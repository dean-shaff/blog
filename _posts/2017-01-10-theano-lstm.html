---
layout: post
title: Theano LSTM
date: '2017-01-10T10:01:00.001-08:00'
author: dshaff001
tags: 
modified_time: '2017-01-10T10:12:49.438-08:00'
blogger_id: tag:blogger.com,1999:blog-8551793629529973046.post-8830212994132069137
blogger_orig_url: http://deanshaff.blogspot.com/2017/01/theano-lstm.html
---

<div dir="ltr" style="text-align: left;" trbidi="on">I'm just going to show a code snippet the shows the main forward step of a single layer Long Short Term Memory (LSTM) recurrent neural network (RNN). I'm not going to go through how it all works, as there are a ton of great resources online for RNNs. Check out <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">this</a>, by Andrej Karpathy, <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">this tutorial</a> on implementing RNNs in NumPy and Theano, or <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">this</a> fantastic explanation of LSTMs. Note that I've gotten this to work with the MNIST dataset, resulting in some crazy low error rates. If you're interested in seeing that code, let me know! <br /><br /><script src="https://gist.github.com/dean-shaff/b1b53bd95dc48af0db0422a95c9db884.js"></script><br /><br /></div>